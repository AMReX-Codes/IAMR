
NOTES
1. In GODUNOV - I did not update BDS. I don't think it will work with my tiling updates.
2. For loops that deal with the intersection of coarse and fine grids, 2 have OMP only, 2 have tiling turned on.  See below, Item B under "Good without tiling." Perhaps best way is problem dependent? (Find these loops by grepping for 'intersections'.)
3. These work but could perhaps be done a cleaner way, see comments in code
   MacOperator.cpp:106:    for (MFIter rhomfi(rho); rhomfi.isValid(); ++rhomfi)
   MacOperator.cpp:309:    for (MFIter Phimfi(Phi); Phimfi.isValid(); ++Phimfi)
4. SyncRegister.cpp - OrientationIter inside OMP region? Similar loops
   ViscBndry.cpp:102:    for (OrientationIter fi; fi; ++fi)
   ViscBndryTensor.cpp:76:    for (OrientationIter fi; fi; ++fi)
5. OMP for BoxList::iterator, FabSet iterator???


!------Loops not finished-----------------------------------------------

! complicated coarse-fine interaction; loops over interp functions.  
   !!SyncInterp functions - coarse-fine interaction. Got these wrong earlier
     // This is very similar to InterpFromCoarseLevel() in
     // amrex/Src/AmrCore/AMReX_FillPatchUtil.cpp, which does not use tiling
     // see more comments in file
NavierStokesBase.cpp:3251:    for (MFIter mfi(cdataMF); mfi.isValid(); ++mfi)
NavierStokesBase.cpp:3368:        for (MFIter mfi(crse_phi); mfi.isValid(); ++mfi)
NavierStokesBase.cpp:3383:        for (MFIter mfi(crse_phi); mfi.isValid(); ++mfi)

! Init from velocity file
NavierStokes.cpp:175:            for (MFIter mfi(tmp); mfi.isValid(); ++mfi)

! anelastic stuff - not looked at
MacProj.cpp:1628:    for (MFIter mfi(*area); mfi.isValid(); ++mfi)
Projection.cpp:1921:    for (MFIter mfmfi(mf); mfmfi.isValid(); ++mfmfi) 
Projection.cpp:1956:    for (MFIter mfmfi(mf); mfmfi.isValid(); ++mfmfi) 

! Analysis stuff - getting moved out?
NavierStokesBase.cpp:3876:    for (MFIter mfi(*mf); mfi.isValid(); ++mfi)
NavierStokesBase.cpp:4027:    for (MFIter turbMfi(*turbMF), presMfi(*presMF);
NavierStokesBase.cpp:4049:    for (MFIter turbMfi(*turbMF), presMfi(*presMF);
NavierStokesBase.cpp:4093:    for (MFIter turbMfi(*turbMF), presMfi(*presMF);
NavierStokesBase.cpp:4115:    for (MFIter turbMfi(*turbMF), presMfi(*presMF);
  ! maxVal - only used for mag_vort in sum_integrated quantities analysis stuff
NavierStokes.cpp:902:    for (MFIter mfi(*mf); mfi.isValid(); ++mfi)


!------Loops I think are good without tiling-----------------------------------------------

A. Loops over thin boundary region.  No OMP
  1.  MacProj.cpp:1549:  for (MFIter mfi(u_mac[dim]); mfi.isValid(); ++mfi)	
        In test_umac_periodic() which by default is only for DEBUG
        Also, complication because inside loop is std::vector::push_back(), which isn't
        thread safe, I think      
  2.  NavierStokesBase.cpp:2283:  for (MFIter tbi(tags); !hasTags && tbi.isValid(); ++tbi)
        Thin boudnary region, and likely early exit from mfiter loop


B. Loops dealing with coarse-fine interaction. 2 OMP only, 2 OMP & tiling
   OMP only
   1. NavierStokesBase.cpp:1087:            for (MFIter mfi(crse_src); mfi.isValid(); ++mfi)
        In create_umac_grown(), boxes already divided because crse_src is coarsened from the fine ba 
   2. NavierStokes.cpp:1757:    for (MFIter Vsyncmfi(Vsync); Vsyncmfi.isValid(); ++Vsyncmfi)
        Loop over coarse-fine boxArray intersection
      
   OMP and tiling - these got tiled before we settled on whether to tile or not.
   3. MacProj.cpp:537:            for (MFIter Rhsmfi(Rhs); Rhsmfi.isValid(); ++Rhsmfi)
   4. MacProj.cpp:740:            for (MFIter Rhsmfi(Rhs); Rhsmfi.isValid(); ++Rhsmfi)
        In mac_sync_solve(), if ( solver != mlmg && mac.fix_mac_sync_rhs) (IAMR)
	if (solver!=mlmg && (fix_mac_sync_rhs || subtract_avg)) (LMC)
 	Zeros area of coarse grid covered by fine, one version for IAMR and the 2nd for
	LMC closed chamber
  

C. Loops/work over thin boundary regions.  All have OMP, but perhaps not all need it
   NavierStokesBase.cpp:1115:            for (MFIter mfi(fine_src); mfi.isValid(); ++mfi)
   NavierStokesBase.cpp:1158:        for (MFIter mfi(u_mac[n]); mfi.isValid(); ++mfi)
   MacProj.cpp:1465:            for (MFIter mfi(*mac_phi); mfi.isValid(); ++mfi)
   Projection.cpp:2169:            for (MFIter mfi(phi_crse_strip); mfi.isValid(); ++mfi)
   Projection.cpp:2471:        for (MFIter mfi(phi_fine_strip_mf); mfi.isValid(); ++mfi) {
   Projection.cpp:2996:	for (MFIter mfi(msk); mfi.isValid(); ++mfi)
   Projection.cpp:3053:	for (MFIter mfi(*vel[lev]); mfi.isValid(); ++mfi) {
   SyncRegister.cpp:87:	for (MFIter mfi(rhs); mfi.isValid(); ++mfi)
   SyncRegister.cpp:283:      for (MFIter mfi(Sync_resid_fine); mfi.isValid(); ++mfi)
   SyncRegister.cpp:337:	    for (MFIter mfi(Sync_resid_fine); mfi.isValid(); ++mfi)

D. setPhysBoundaryValues() No OMP with these
   Projection.cpp:264:    for (MFIter mfi(S_new); mfi.isValid(); ++mfi)
   Projection.cpp:942:            for (MFIter mfi(S_new); mfi.isValid(); ++mfi) {
   Projection.cpp:975:        for (MFIter mfi(S_new); mfi.isValid(); ++mfi)
   Projection.cpp:1000:            for (MFIter mfi(divu_new); mfi.isValid(); ++mfi)
   Projection.cpp:1135:        for (MFIter mfi(S_new); mfi.isValid(); ++mfi)
   Projection.cpp:1284:            for (MFIter mfi(divu_new); mfi.isValid(); ++mfi)
   Projection.cpp:1346:        for (MFIter mfi(S_new); mfi.isValid(); ++mfi)

